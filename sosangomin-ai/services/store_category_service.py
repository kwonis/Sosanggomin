# services/store_category_service.py

import sys
import os
import logging
import asyncio
import aiohttp
from datetime import datetime
from dotenv import load_dotenv
from db_models import StoreCategories  
import json
import pandas as pd
from sqlalchemy.orm import Session
from sqlalchemy import func, desc
from typing import Optional, Dict, List, Any

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
logger = logging.getLogger(__name__)

class StoreCategoryService:
    def __init__(self):
        load_dotenv("./config/.env")
        
        self.store_api_key = os.getenv("STORE_API_KEY")
        self.change_api_key = os.getenv("TRADE_AREA_CHANGE_KEY")
        self.base_url = "http://openapi.seoul.go.kr:8088"
        self.store_service_name = "VwsmAdstrdStorW"
        self.change_service_name = "VwsmAdstrdIxQq"

        base_dir = os.path.dirname(os.path.abspath(__file__))
        json_dir = os.path.join(base_dir, "..", "data")

        with open(os.path.join(json_dir, "seoul_dong_to_district.json"), "r", encoding="utf-8") as f:
            self.dong_to_district = json.load(f)

        with open(os.path.join(json_dir, "industry_to_main_category.json"), "r", encoding="utf-8") as f:
            self.industry_to_main_category = json.load(f)

        if not self.store_api_key:
            logger.error("ì í¬ ì¸ì¦í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

        if not self.change_api_key:
            logger.error("ìƒê¶Œë³€í™”ì§€í‘œ ì¸ì¦í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    def get_main_category(self, industry_name: str) -> str:
        """ì—…ì¢…ëª…ì„ ëŒ€ë¶„ë¥˜ë¡œ ë³€í™˜"""
        for main_cat, names in self.industry_to_main_category.items():
            if industry_name in names:
                return main_cat
        return "ê¸°íƒ€"
    
    def safe_get(self, row, key):
        val = row.get(key)
        return None if pd.isna(val) else val
    
    async def fetch_store_data(self, session) -> dict:
        """ìƒê¶Œ ì í¬ ë°ì´í„° API í˜¸ì¶œ"""
        try:
            all_data = []
            start = 1
            limit = 1000  # í˜ì´ì§€ë‹¹ ìˆ˜
            while True:
                end = start + limit - 1
                url = f"{self.base_url}/{self.store_api_key}/json/{self.store_service_name}/{start}/{end}/"
                async with session.get(url) as response:
                    response.raise_for_status()
                    data = await response.json()
                    rows = data[self.store_service_name].get("row", [])
                    if not rows:
                        break
                    all_data.extend(rows)
                    if len(rows) < limit:
                        break
                    start += limit
                    await asyncio.sleep(0.2)
            df = pd.DataFrame(all_data)
            if df.empty:
                return pd.DataFrame()

            df["year_quarter"] = df["STDR_YYQU_CD"]
            df["year"] = df["year_quarter"].str[:4].astype(int)
            df["quarter"] = df["year_quarter"].str[4:].astype(int)

            df["region_name"] = df["ADSTRD_CD_NM"].str.replace("Â·", ".", regex=False)
            df["region_name"] = df["region_name"].replace({"ì¼ì›2ë™": "ê°œí¬3ë™"}) # ì¼ì›2ë™ â†’ ê°œí¬3ë™
            df["district_name"] = df["region_name"].map(self.dong_to_district)
            df["industry_name"] = df["SVC_INDUTY_CD_NM"]
            df["main_category"] = df["industry_name"].apply(self.get_main_category)
            df["store_count"] = pd.to_numeric(df["STOR_CO"], errors="coerce").fillna(0)

            # ëŒ€ë¶„ë¥˜ë³„ ì´í•© ê³„ì‚°
            total_df = df.groupby(["year", "quarter", "main_category"])["store_count"].sum().reset_index()
            total_df.columns = ["year", "quarter", "main_category", "main_category_total"]
            df = df.merge(total_df, on=["year", "quarter", "main_category"], how="left")

            return df
        except Exception as e:
            logger.error(f"ìƒê¶Œ ì í¬ API ì˜¤ë¥˜: {e}")
            return pd.DataFrame()
        
    async def fetch_change_data(self, session) -> dict:
        """ìƒê¶Œ ë³€í™” ì§€í‘œ ë°ì´í„° API í˜¸ì¶œ"""
        try:
            all_data = []
            start = 1
            limit = 1000  # í˜ì´ì§€ë‹¹ ìˆ˜
            while True:
                end = start + limit - 1
                url = f"{self.base_url}/{self.change_api_key}/json/{self.change_service_name}/{start}/{end}/"
                async with session.get(url) as response:
                    response.raise_for_status()
                    data = await response.json()
                    rows = data[self.change_service_name].get("row", [])
                    if not rows:
                        break
                    all_data.extend(rows)
                    if len(rows) < limit:
                        break
                    start += limit
                    await asyncio.sleep(0.2)
                
            df = pd.DataFrame(all_data)
            if df.empty:
                return pd.DataFrame()
            
            df["year_quarter"] = df["STDR_YYQU_CD"]
            df["year"] = df["year_quarter"].str[:4].astype(int)
            df["quarter"] = df["year_quarter"].str[4:].astype(int)
            df["region_name"] = df["ADSTRD_CD_NM"].str.replace("Â·", ".", regex=False)
            df["region_name"] = df["region_name"].replace({"ì¼ì›2ë™": "ê°œí¬3ë™"}) # ì¼ì›2ë™ â†’ ê°œí¬3ë™

            return df
        except Exception as e:
            logger.error(f"ìƒê¶Œ ë³€í™” ì§€í‘œ API ì˜¤ë¥˜: {e}")
            return pd.DataFrame()
        
    # async def update_store_data(self):
    #     """ìƒê¶Œ ë¶„ì„ : ì—…ì¢… ë°ì´í„° ìˆ˜ì§‘ ë° DB ì €ì¥"""
    #     from database.connector import database_instance as mariadb
    #     logger.error(f"ğŸ“Œ update_store_data() ì‹¤í–‰ ì‹œì‘ë¨ - ê°ì²´ ID: {id(self)}")

    #     db = mariadb.pre_session()
    #     total_updated = 0
    #     total_saved = 0 

    #     try:
    #         logger.error("ìƒê¶Œ ì í¬ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œì‘")
    #         timeout = aiohttp.ClientTimeout(total=30)  # íƒ€ì„ì•„ì›ƒ ì„¤ì • (API ë¬´í•œëŒ€ê¸° ë°©ì§€)
        
    #         async with aiohttp.ClientSession(timeout=timeout) as session:
    #             store_df = await self.fetch_store_data(session)
    #             logger.error(f"ì í¬ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ - {len(store_df)} rows")  # â— í™•ì¸ìš© ë¡œê·¸

    #             change_df = await self.fetch_change_data(session)                
    #             logger.error(f"ìƒê¶Œ ë³€í™” ì§€í‘œ ìˆ˜ì§‘ ì™„ë£Œ - {len(change_df)} rows")  # â— í™•ì¸ìš© ë¡œê·¸

    #             if store_df.empty:
    #                 logger.warning("ìƒê¶Œ ì í¬ ë°ì´í„° ì—†ìŒ")
    #                 return
                
    #             if change_df.empty:
    #                 logger.warning("ìƒê¶Œ ë³€í™” ì§€í‘œ ë°ì´í„° ì—†ìŒ")
    #                 return

    #             # ìƒê¶Œ ë³€í™” ì§€í‘œ ë³‘í•©
    #             merged_df = pd.merge(
    #                 store_df,
    #                 change_df,
    #                 on=["year", "quarter", "region_name"],
    #                 how="left"
    #             )

    #             # ì €ì¥ ì „ NaN ë¹„êµ ê¸°ì¤€ í•„ë“œ
    #             important_fields = [
    #                 "OPBIZ_RT", "OPBIZ_STOR_CO", "CLSBIZ_RT", "CLSBIZ_STOR_CO",
    #                 "TRDAR_CHNGE_IX", "TRDAR_CHNGE_IX_NM",
    #                 "OPR_SALE_MT_AVRG", "CLS_SALE_MT_AVRG",
    #                 "SU_OPR_SALE_MT_AVRG", "SU_CLS_SALE_MT_AVRG"
    #             ]

    #             for i, (_, row) in enumerate(merged_df.iterrows()):
    #                 try:
    #                     # ì§„í–‰ ìƒí™© í™•ì¸ ë¡œê·¸ (100ê°œë§ˆë‹¤)
    #                     if i % 100 == 0:
    #                         logger.info(f"{i}ë²ˆì§¸ row ì²˜ë¦¬ ì¤‘...")

    #                     year, quarter = row["year"], row["quarter"]
    #                     region_name = row["region_name"]
    #                     district_name = row["district_name"]
    #                     industry_name = row["industry_name"]
                        
    #                     existing = db.query(StoreCategories).filter_by(
    #                         year=year,
    #                         quarter=quarter,
    #                         region_name=region_name,
    #                         industry_name=industry_name
    #                     ).first()

    #                     # í˜„ì¬ ë°ì´í„° NaNì¸ì§€ í™•ì¸
    #                     current_all_nan = all(pd.isna(row.get(field)) for field in important_fields)
                        
    #                     # ì´ë¯¸ DBì— ì €ì¥ëœ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ë‹¤ë©´, í˜¹ì€ ìƒˆë¡œ ë°›ì€ ë°ì´í„°ë„ ì•„ë¬´ ì˜ë¯¸ ì—†ëŠ” ê°’ì´ë¼ë©´, ê±´ë„ˆë›°ê¸°
    #                     if existing:
    #                         existing_has_all_data = all([
    #                             getattr(existing, field.lower(), None) is not None for field in important_fields
    #                         ])
    #                         if existing_has_all_data or current_all_nan:
    #                             continue  # ì €ì¥ ìƒëµ

    #                     main_category = row["main_category"]
    #                     store_count = row["store_count"]
    #                     main_category_total = row["main_category_total"]

    #                     open_rate = self.safe_get(row, "OPBIZ_RT")
    #                     open_store_count = self.safe_get(row, "OPBIZ_STOR_CO")
    #                     close_rate = self.safe_get(row, "CLSBIZ_RT")
    #                     close_store_count = self.safe_get(row, "CLSBIZ_STOR_CO")

    #                     ta_change_index = self.safe_get(row, "TRDAR_CHNGE_IX")
    #                     ta_change_index_name = self.safe_get(row, "TRDAR_CHNGE_IX_NM")
    #                     open_sales_month_avg = self.safe_get(row, "OPR_SALE_MT_AVRG")
    #                     close_sales_month_avg = self.safe_get(row, "CLS_SALE_MT_AVRG")
    #                     seoul_open_sales_month_avg = self.safe_get(row, "SU_OPR_SALE_MT_AVRG")
    #                     seoul_close_sales_month_avg = self.safe_get(row, "SU_CLS_SALE_MT_AVRG")


    #                     if existing:
    #                         existing.district_name = district_name
    #                         existing.main_category = main_category
    #                         existing.store_count = store_count
    #                         existing.main_category_total = main_category_total
    #                         existing.open_rate = open_rate
    #                         existing.open_store_count = open_store_count
    #                         existing.close_rate = close_rate
    #                         existing.close_store_count = close_store_count
    #                         existing.ta_change_index = ta_change_index
    #                         existing.ta_change_index_name = ta_change_index_name
    #                         existing.open_sales_month_avg = open_sales_month_avg
    #                         existing.close_sales_month_avg = close_sales_month_avg
    #                         existing.seoul_open_sales_month_avg = seoul_open_sales_month_avg
    #                         existing.seoul_close_sales_month_avg = seoul_close_sales_month_avg
    #                         existing.updated_at = datetime.now()
    #                         db.commit()
    #                         total_updated += 1
    #                     else:
    #                         new_record = StoreCategories(
    #                             year=year,
    #                             quarter=quarter,
    #                             district_name=district_name,
    #                             region_name=region_name,
    #                             industry_name=industry_name,
    #                             main_category=main_category,
    #                             store_count=store_count,
    #                             main_category_total=main_category_total,
    #                             open_rate=open_rate,
    #                             open_store_count=open_store_count,
    #                             close_rate=close_rate,
    #                             close_store_count=close_store_count,
    #                             ta_change_index=ta_change_index,
    #                             ta_change_index_name=ta_change_index_name,
    #                             open_sales_month_avg=open_sales_month_avg,
    #                             close_sales_month_avg=close_sales_month_avg,
    #                             seoul_open_sales_month_avg=seoul_open_sales_month_avg,
    #                             seoul_close_sales_month_avg=seoul_close_sales_month_avg,
    #                             created_at=datetime.now()
    #                         )
    #                         db.add(new_record)
    #                         db.commit()
    #                         total_saved += 1
    #                 except Exception as e:
    #                     db.rollback()
    #                     logger.warning(f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

    #             db.commit()
    #             logger.error(f"ì €ì¥ ì™„ë£Œ - ì‹ ê·œ {total_saved}ê±´, ìˆ˜ì • {total_updated}ê±´")

    #     except Exception as e:
    #         db.rollback()
    #         logger.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
    #     finally:
    #         db.close()
    async def update_store_data(self):
        """ìƒê¶Œ ë¶„ì„ : ì—…ì¢… ë°ì´í„° ìˆ˜ì§‘ ë° DB ì €ì¥ (bulk ì €ì¥ + UPSERT ë°©ì‹ ì ìš©)"""
        from database.connector import database_instance as mariadb
        db = mariadb.pre_session()
        total_saved, total_updated = 0, 0

        try:
            logger.info("ìƒê¶Œ ì í¬ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œì‘")
            timeout = aiohttp.ClientTimeout(total=30)

            async with aiohttp.ClientSession(timeout=timeout) as session:
                store_df = await self.fetch_store_data(session)
                logger.info(f"ì í¬ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(store_df)} rows")

                change_df = await self.fetch_change_data(session)
                logger.info(f"ìƒê¶Œ ë³€í™” ì§€í‘œ ìˆ˜ì§‘ ì™„ë£Œ: {len(change_df)} rows")

                if store_df.empty or change_df.empty:
                    logger.warning("ì í¬ ë° ìƒê¶Œ ë³€í™” ì§€í‘œì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŒ")
                    return

                # ìƒê¶Œ ë³€í™” ì§€í‘œ ë³‘í•©
                merged_df = pd.merge(store_df, change_df, on=["year", "quarter", "region_name"], how="left")

                # ê¸°ì¡´ DBì—ì„œ (year, quarter, region_name, industry_name) ì¡°í•© ë¡œë”©
                existing_rows = db.query(StoreCategories).all()
                existing_map = {
                    (r.year, r.quarter, r.region_name, r.industry_name): r for r in existing_rows
                }

                insert_list = []
                update_list = []

                for i, (_, row) in enumerate(merged_df.iterrows()):
                    if i % 200 == 0:
                        logger.info(f"{i}ë²ˆì§¸ row ì²˜ë¦¬ ì¤‘...")

                    key = (row["year"], row["quarter"], row["region_name"], row["industry_name"])
                    base_data = {
                        "year": row["year"],
                        "quarter": row["quarter"],
                        "district_name": row["district_name"],
                        "region_name": row["region_name"],
                        "industry_name": row["industry_name"],
                        "main_category": row["main_category"],
                        "store_count": row["store_count"],
                        "main_category_total": row["main_category_total"],
                        "open_rate": self.safe_get(row, "OPBIZ_RT"),
                        "open_store_count": self.safe_get(row, "OPBIZ_STOR_CO"),
                        "close_rate": self.safe_get(row, "CLSBIZ_RT"),
                        "close_store_count": self.safe_get(row, "CLSBIZ_STOR_CO"),
                        "ta_change_index": self.safe_get(row, "TRDAR_CHNGE_IX"),
                        "ta_change_index_name": self.safe_get(row, "TRDAR_CHNGE_IX_NM"),
                        "open_sales_month_avg": self.safe_get(row, "OPR_SALE_MT_AVRG"),
                        "close_sales_month_avg": self.safe_get(row, "CLS_SALE_MT_AVRG"),
                        "seoul_open_sales_month_avg": self.safe_get(row, "SU_OPR_SALE_MT_AVRG"),
                        "seoul_close_sales_month_avg": self.safe_get(row, "SU_CLS_SALE_MT_AVRG"),
                        "updated_at": datetime.now()
                    }

                    if key in existing_map:
                        # update
                        base_data["store_statistic_id"] = existing_map[key].store_statistic_id
                        update_list.append(base_data)
                        total_updated += 1
                    else:
                        # insert
                        base_data["created_at"] = datetime.now()
                        insert_list.append(StoreCategories(**base_data))
                        total_saved += 1

                # ì €ì¥ ì²˜ë¦¬
                if insert_list:
                    db.bulk_save_objects(insert_list)

                if update_list:
                    db.bulk_update_mappings(StoreCategories, update_list)

                db.commit()
                logger.error(f"ìƒê¶Œ ë¶„ì„ : ì—…ì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ - ì‹ ê·œ {total_saved}ê±´, ìˆ˜ì • {total_updated}ê±´")

        except Exception as e:
            db.rollback()
            logger.error(f"ì—…ì¢… ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        finally:
            db.close()

store_category_service = StoreCategoryService()

# ë‹¨ë… ì‹¤í–‰ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    async def main():
        await store_category_service.update_store_data()
    asyncio.run(main())
