# sevices/auto_analysis.py

import os
import sys
import logging
import pandas as pd
import numpy as np
from datetime import datetime
from bson import ObjectId
import holidays  #type: ignore
from prophet import Prophet #type: ignore
from pmdarima import auto_arima
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, silhouette_score
from services.weather_service import weather_service

from database.mongo_connector import mongo_instance
from services.s3_service import download_file_from_s3
from services.auto_analysis_chat_service import autoanalysis_chat_service

# ìš°ì„  í‚¤ì›€ í˜ì´ í¬ìŠ¤ê¸° ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ìŒ.
os.environ["LOKY_MAX_CPU_COUNT"] = "8"
logger = logging.getLogger(__name__)

class AutoAnalysisService: 
    def __init__(self): 
        self.temp_dir = "temp_files"
        os.makedirs(self.temp_dir, exist_ok=True)

        self.pos_col = {
            "í‚¤ì›€": {
                "datetime": "ë§¤ì¶œ ì¼ì‹œ",
                "qty": "ìˆ˜ëŸ‰",
                "price": "ë‹¨ê°€",
                "product": "ìƒí’ˆ ëª…ì¹­" 
            },
            "í† ìŠ¤": {
                "datetime": "ì£¼ë¬¸ì‹œì‘ì‹œê°",
                "qty": "ìˆ˜ëŸ‰",
                "price": "ìƒí’ˆê°€ê²©",
                "product": "ìƒí’ˆëª…"
            }
        }

    async def read_file(self, temp_file: str, pos_type: str = "í‚¤ì›€") -> pd.DataFrame:
        """íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ë°ì´í„° ì½ê¸°"""
        ext = os.path.splitext(temp_file)[1].lower()
        start = 2 if pos_type == "í‚¤ì›€" else 0
        if ext == ".csv":
            return pd.read_csv(temp_file, header=start)
        elif ext in [".xls", ".xlsx"]:
            return pd.read_excel(temp_file, header=start)
        else:
            raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. CSV ë˜ëŠ” Excelë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

    def validate_and_normalize_pos(self, df: pd.DataFrame, pos_type: str) -> pd.DataFrame:
        """POS ë°ì´í„°ì˜ í˜•ì‹ ê²€ì¦ ë° í‘œì¤€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ í†µì¼"""
        pos_map = self.pos_col[pos_type]
        required_cols = list(pos_map.values())

        # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        missing = [col for col in required_cols if col not in df.columns]
        if missing:
            raise ValueError(
                f"'{pos_type}' POS í˜•ì‹ìœ¼ë¡œ ë¶„ì„ì„ ì‹œë„í–ˆìœ¼ë‚˜, í•„ìˆ˜ ì»¬ëŸ¼ {missing} ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                "ì„ íƒí•œ POS ìœ í˜•ì´ ì‹¤ì œ ë°ì´í„°ì™€ ë‹¤ë¥´ê±°ë‚˜, ì—…ë¡œë“œëœ íŒŒì¼ì´ POS í˜•ì‹ì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                "íŒŒì¼ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”."
            )
        # í‘œì¤€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ í†µì¼
        df = df.rename(columns={
            pos_map["datetime"]: "ë§¤ì¶œ ì¼ì‹œ",
            pos_map["qty"]: "ìˆ˜ëŸ‰",
            pos_map["price"]: "ë‹¨ê°€",
            pos_map["product"]: "ìƒí’ˆ ëª…ì¹­"
        })

        # ìˆ«ìí˜• ë³€í™˜
        df['ìˆ˜ëŸ‰'] = pd.to_numeric(df['ìˆ˜ëŸ‰'], errors='coerce')
        df['ë‹¨ê°€'] = pd.to_numeric(df['ë‹¨ê°€'], errors='coerce')
        
        # ë‚ ì§œ í˜•ì‹ í™•ì¸
        df['ë§¤ì¶œ ì¼ì‹œ'] = pd.to_datetime(df['ë§¤ì¶œ ì¼ì‹œ'], format="%Y-%m-%d %H:%M:%S", errors='coerce')
        
        # í–‰ ìˆ˜ ë„ˆë¬´ ì ìœ¼ë©´ ë¹„ì •ìƒ íŒŒì¼ ê°€ëŠ¥ì„±
        if df.shape[0] < 5:
            raise ValueError(f"í–‰ ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {df.shape[0]}í–‰")

        # ìƒí’ˆëª… ë‹¤ì–‘ì„± ê²€ì‚¬
        if df['ìƒí’ˆ ëª…ì¹­'].nunique() <= 1:
            raise ValueError(f"ìƒí’ˆëª…ì´ 1ê°œ ì´í•˜ì…ë‹ˆë‹¤.")

        return df

    async def preprocess_data(self, df: pd.DataFrame, pos_type: str = "í‚¤ì›€") -> pd.DataFrame:
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° ì‹œê°„ ë³€ìˆ˜ ìƒì„±"""
        try:

            # TODO: ê²°ì œ ìˆ˜ë‹¨ ì´ìš©í• ê±´ì§€?
            if pos_type == "í‚¤ì›€":

                # í—¤ë”ì˜ ë³€ìˆ˜ëª…ê³¼ ê°™ì€ ê°’ì„ ê°€ì§€ëŠ” ì—´ì„ ì‚­ì œ 
                header_values = set(df.columns.tolist()) 
                columns_to_drop = [col for col in df.columns if any(df[col].astype(str).isin(header_values))]
                df = df.drop(columns=columns_to_drop)

                if df.empty:
                    raise ValueError("ì „ì²˜ë¦¬ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì—‘ì…€ íŒŒì¼ì˜ êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                # ê²°ì¸¡ ë° ì¤‘ë³µ ì²˜ë¦¬ 
                # df = (
                #     df.dropna(axis=0, how='all')  # ëª¨ë“  ê°’ì´ NaNì¸ í–‰ ì œê±°
                #     .dropna(axis=1, how='all')  # ëª¨ë“  ê°’ì´ NaNì¸ ì—´ ì œê±°
                #     .loc[:, df.nunique() > 1]  # ê³ ìœ ê°’ 1ê°œ ì´í•˜ì¸ ì—´ ì œê±°
                #     .T.drop_duplicates().T    # ì¤‘ë³µ ì—´ ì œê±°
                # )

                logger.info(f"[ì´ˆê¸° df shape]: {df.shape}")

                # ëª¨ë“  NaN í–‰ ì œê±°
                df = df.dropna(axis=0, how='all')
                logger.info(f"[NaN í–‰ ì œê±° í›„]: {df.shape}")

                # ëª¨ë“  NaN ì—´ ì œê±°
                df = df.dropna(axis=1, how='all')
                logger.info(f"[NaN ì—´ ì œê±° í›„]: {df.shape}")

                # ê³ ìœ ê°’ 1ê°œ ì´í•˜ ì»¬ëŸ¼ ì œê±°
                df = df.loc[:, df.nunique() > 1]
                logger.info(f"[ê³ ìœ ê°’ 1ê°œ ì´í•˜ ì»¬ëŸ¼ ì œê±° í›„]: {df.shape}")

                # ì¤‘ë³µ ì—´ ì œê±°
                df = df.T.drop_duplicates().T
                logger.info(f"[ì¤‘ë³µ ì—´ ì œê±° í›„]: {df.shape}")

                if df.empty:
                    raise ValueError("ì „ì²˜ë¦¬ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì—‘ì…€ íŒŒì¼ì˜ êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                # 'Unnamed'ê°€ í¬í•¨ë˜ì§€ ì•Šì€ ì—´ ì¤‘ë³µ
                cols_to_fill = [col for col in df.columns if 'Unnamed' not in str(col)]
                df[cols_to_fill] = df[cols_to_fill].ffill()

                # ë™ì¼ ì†ì„±ì´ ì—¬ëŸ¬ ë‹¤ë¥¸ ì¹¼ëŸ¼ì— ì¡´ì¬í•˜ëŠ” ê²½ìš°, ì´ë¥¼ í•˜ë‚˜ì˜ ì¹¼ëŸ¼ìœ¼ë¡œ ì •ë¦¬
                dup_val = ['ë‹¨ê°€', 'ìˆ˜ëŸ‰', 'ì›ê°€', 'ê±°ìŠ¤ë¦„']
                for val in dup_val :
                    columns = [col for col in df.columns if df[col].astype(str).str.contains(val, na=False).any()]
                    if columns:
                        df[val] = df[columns].bfill(axis=1).iloc[:, 0] 
                        df = df.drop(columns=columns)
                if "ê±°ìŠ¤ë¦„" in df.columns:
                    df.drop(columns=['ê±°ìŠ¤ë¦„'], inplace=True)
                if "ì›ê°€" in df.columns:
                    df.drop(columns=['ì›ê°€'], inplace=True)

                logger.error(f'[df.dropna] ì²˜ë¦¬ ì „ {len(df)}')
                df = df.dropna(axis=0, how='any') # ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ ì œê±°
                logger.error(f'[df.dropna] ì²˜ë¦¬ í›„ {len(df)}')

                if df.shape[0] == 0:
                    logger.error("âŒ ì»¬ëŸ¼ëª… ì²˜ë¦¬ ì§ì „ì— ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ìˆìŒ. ì—´ ì´ë¦„ ì¶”ì¶œ ë¶ˆê°€.")
                    raise ValueError("ì»¬ëŸ¼ëª… ì²˜ë¦¬ ì „ì— ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

                logger.info(f"[ì»¬ëŸ¼ëª… ì²˜ë¦¬ ì‹œì‘] df.shape: {df.shape}, columns: {df.columns.tolist()}")

                try:
                    new_columns = [df.iloc[0, i] if 'Unnamed' in str(col) else col for i, col in enumerate(df.columns)]
                except Exception as e:
                    logger.error(f"â— new_columns ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    raise

                df.columns = new_columns
                logger.info(f"[ì „ì²˜ë¦¬ ì „] df shape: {df.shape}")
                df = df[~df.apply(lambda row: any(row.astype(str).isin(new_columns)), axis=1)]
                logger.info(f"[ì»¬ëŸ¼ ì •ë¦¬ í›„] df shape: {df.shape}")

                if df.empty:
                    logger.warning("ì»¬ëŸ¼ëª… ì²˜ë¦¬ í›„ ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                    raise ValueError("ì»¬ëŸ¼ëª… ì²˜ë¦¬ í›„ ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë°ì´í„° êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                
                df = df.loc[:, df.nunique() > 1]  

                if df.empty:
                    logger.warning("ì»¬ëŸ¼ëª… ì²˜ë¦¬ í›„ ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                    raise ValueError("ì»¬ëŸ¼ëª… ì²˜ë¦¬ í›„ ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë°ì´í„° êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                
                # ë§¤ì¶œ 
                df = df.drop(columns=['ì´ë§¤ì¶œ', 'ì‹¤ë§¤ì¶œ'], errors='ignore') 
                df['ë§¤ì¶œ'] = (df['ë‹¨ê°€'] * df['ìˆ˜ëŸ‰']).astype(int)
            
            # íŒŒì¼ í˜•ì‹ í™•ì¸
            df = self.validate_and_normalize_pos(df, pos_type)

            if pos_type == "í† ìŠ¤":
                df['ë§¤ì¶œ'] = df['ë‹¨ê°€']
                df['ë‹¨ê°€'] = df['ë§¤ì¶œ'] / df['ìˆ˜ëŸ‰']
                df = df.drop(index=0).reset_index(drop=True)
                df = (
                    df.dropna(axis=0, how='all')  # ëª¨ë“  ê°’ì´ NaNì¸ í–‰ ì œê±°
                    .dropna(axis=1, how='all')  # ëª¨ë“  ê°’ì´ NaNì¸ ì—´ ì œê±°
                    .loc[:, df.nunique() > 1]  # ê³ ìœ ê°’ 1ê°œ ì´í•˜ì¸ ì—´ ì œê±°
                    .T.drop_duplicates().T    # ì¤‘ë³µ ì—´ ì œê±°
                )

            if df['ìˆ˜ëŸ‰'].isna().any():
                raise ValueError("'ìˆ˜ëŸ‰' ì»¬ëŸ¼ì— ìˆ«ìë¡œ ë³€í™˜í•  ìˆ˜ ì—†ëŠ” ê°’ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        
            # íŒŒìƒë³€ìˆ˜ ìƒì„±
            kr_holidays = holidays.KR()

            df['ë§¤ì¶œ ì¼ì‹œ'] = pd.to_datetime(df['ë§¤ì¶œ ì¼ì‹œ'])
            df['ë…„'] = df['ë§¤ì¶œ ì¼ì‹œ'].dt.year.astype(str)  
            df['ì›”'] = df['ë§¤ì¶œ ì¼ì‹œ'].dt.month.astype(str).str.zfill(2)
            df['ì¼'] = df['ë§¤ì¶œ ì¼ì‹œ'].dt.day.astype(str).str.zfill(2)
            df['ì‹œ'] = df['ë§¤ì¶œ ì¼ì‹œ'].dt.hour.astype(str).str.zfill(2)
            df['ë¶„'] = df['ë§¤ì¶œ ì¼ì‹œ'].dt.minute.astype(str).str.zfill(2)
            df['ìš”ì¼'] = df['ë§¤ì¶œ ì¼ì‹œ'].dt.day_name()  
            df['ì‹œê°„ëŒ€'] = df['ì‹œ'].astype(int).apply(lambda x: 'ì ì‹¬' if 11 <= x <= 15 else ('ì €ë…' if 17 <= x <= 21 else 'ê¸°íƒ€')) # ì‹œê°„ëŒ€ (ì ì‹¬, ì €ë…, ê¸°íƒ€)
            df['ê³„ì ˆ'] = df['ì›”'].astype(int).apply(lambda x: 'ë´„' if 3 <= x <= 5 else
                                                'ì—¬ë¦„' if 6 <= x <= 8 else
                                                'ê°€ì„' if 9 <= x <= 11 else 'ê²¨ìš¸')
            df['ê³µíœ´ì¼'] = df['ë§¤ì¶œ ì¼ì‹œ'].dt.date.apply(lambda x: 'íœ´ì¼' if x in kr_holidays or x.weekday() >= 5 else 'í‰ì¼')
            
            # ì™¸ë¶€ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
            min_date, max_date = df['ë§¤ì¶œ ì¼ì‹œ'].min(), df['ë§¤ì¶œ ì¼ì‹œ'].max()
            start_date, end_date = min_date.strftime('%Y%m%d%H'), max_date.strftime('%Y%m%d%H')

            # ë‚ ì”¨
            weather_df = await weather_service.process_weather(start_date, end_date, "ì„œìš¸") # TODO : ì¥ì†Œ ë°›ì•„ì˜¤ëŠ” ë¡œì§ ì§œê¸°

            merged_df = pd.merge(
                df, 
                weather_df, 
                left_on=['ë…„', 'ì›”', 'ì¼', 'ì‹œ'],
                right_on=['year', 'month', 'day', 'hour'],
                how='left'
            ).drop(columns=['year', 'month', 'day', 'hour'])
            
            merged_df = merged_df.rename(columns={
                'ta': 'ê¸°ì˜¨',
                'ws': 'í’ì†',
                'hm': 'ìŠµë„',
                'rn': 'ê°•ìˆ˜ëŸ‰'
            })
            merged_df['ê°•ìˆ˜ëŸ‰'] = merged_df['ê°•ìˆ˜ëŸ‰'].fillna(0)
            # df['ë¯¸ì„¸ë¨¼ì§€']
            # df['ìœ ë™ì¸êµ¬']


            # df.drop('ë§¤ì¶œ ì¼ì‹œ', axis=1, inplace=True)
            # merged_df.columns = ['ë§¤ì¶œ' if col in ['ì´ë§¤ì¶œ', 'ì‹¤ë§¤ì¶œ'] else col for col in merged_df.columns]

            return merged_df
        
        except Exception as e:
            logger.error(f"ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise ValueError(f"ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

    def find_best_k(self, data: pd.DataFrame, k_min: int = 2, k_max: int = 10) -> int:
        """Silhouette Scoreë¡œ ìµœì  k ì°¾ê¸°"""
        best_k = k_min
        best_score = -1

        for k in range(k_min, k_max + 1):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
            labels = kmeans.fit_predict(data)
            score = silhouette_score(data, labels)
            # print(f"[K={k}] Silhouette Score: {score:.4f}")

            if score > best_score:
                best_score = score
                best_k = k

        # print(f"ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {best_k}")
        return best_k

    def find_best_k_elbow(self, data: pd.DataFrame, k_min: int = 2, k_max: int = 10) -> int:
        """ì—˜ë³´ìš° ë°©ë²•(WCSS ê¸°ë°˜)ìœ¼ë¡œ ìµœì  k ì°¾ê¸°"""
        wcss = []

        for k in range(k_min, k_max + 1):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
            kmeans.fit(data)
            wcss.append(kmeans.inertia_)
            print(f"[K={k}] WCSS (êµ°ì§‘ ë‚´ ì œê³±í•©): {kmeans.inertia_:.4f}")

        # ì°¨ë¶„ì„ í†µí•´ ê¸°ìš¸ê¸° ë³€í™” í™•ì¸ (ìë™í™”ëœ ì—˜ë³´ìš° í¬ì¸íŠ¸ íƒìƒ‰)
        deltas = np.diff(wcss)  
        delta_ratios = np.abs(np.diff(wcss) / wcss[:-1]) 

        # ë³€í™”ìœ¨ì´ ê¸‰ê²©í•˜ê²Œ ì¤„ì–´ë“œëŠ” ì§€ì  ì°¾ê¸° (ê°€ì¥ í° ë³€í™” ì´í›„ k ì„ íƒ)
        if len(delta_ratios) > 0:
            elbow_point = np.argmin(delta_ratios) + k_min + 1 
        else:
            elbow_point = k_min  # ë°ì´í„° ì ì„ ë•Œ

        return elbow_point

    async def predict_next_30_sales(self, df: pd.DataFrame, model_type="Prophet"):
        """í–¥í›„ 30ì¼ ë§¤ì¶œ ì˜ˆì¸¡"""
        try:
            # ë‚ ì§œ, ë§¤ì¶œë§Œ ì¶”ì¶œ
            df['ë§¤ì¶œ ì¼ì‹œ'] = pd.to_datetime(df['ë§¤ì¶œ ì¼ì‹œ'])
            sales_df = df[['ë§¤ì¶œ ì¼ì‹œ', 'ë§¤ì¶œ']].copy()

            # ì¼ë³„ ë§¤ì¶œ ì§‘ê³„ 
            daily_sales_df = sales_df.groupby(sales_df['ë§¤ì¶œ ì¼ì‹œ'].dt.date).agg({'ë§¤ì¶œ': 'sum'}).reset_index()
            daily_sales_df = daily_sales_df.rename(columns={'ë§¤ì¶œ ì¼ì‹œ': 'ë‚ ì§œ'})

            # ëˆ„ë½ëœ ë‚ ì§œ ì±„ìš°ê¸°
            date_range = pd.date_range(daily_sales_df['ë‚ ì§œ'].min(), daily_sales_df['ë‚ ì§œ'].max(), freq='D')
            daily_sales_df = daily_sales_df.set_index('ë‚ ì§œ').reindex(date_range, fill_value=daily_sales_df['ë§¤ì¶œ'].mean()).rename_axis('ë‚ ì§œ').reset_index()
           
            # ìš”ì¼ ë³€ìˆ˜ ì¶”ê°€ (One-Hot Encoding)
            daily_sales_df['ìš”ì¼'] = daily_sales_df['ë‚ ì§œ'].dt.dayofweek  # ì›”=0~ì¼=6
            kr_holidays = holidays.KR()
            daily_sales_df['ê³µíœ´ì¼'] = daily_sales_df['ë‚ ì§œ'].apply(lambda x: 1 if x in kr_holidays else 0)

            # í•™ìŠµ ë°ì´í„° ë¶„í•  (ì„±ëŠ¥ í‰ê°€ìš©)
            train_df = daily_sales_df.iloc[:-30]  # í•™ìŠµ ë°ì´í„°
            test_df = daily_sales_df.iloc[-30:]   # í…ŒìŠ¤íŠ¸ ë°ì´í„° (ì„±ëŠ¥ í‰ê°€ìš©)

            # ê²°ì¸¡ì¹˜ ì œê±° (í…ŒìŠ¤íŠ¸ì…‹ ê¸°ì¤€)
            train_df = train_df.dropna(subset=['ë§¤ì¶œ'])

            ### ëª¨ë¸ í•™ìŠµ & ì„±ëŠ¥ í‰ê°€ ###
            df_prophet = train_df.rename(columns={'ë‚ ì§œ': 'ds', 'ë§¤ì¶œ': 'y'})
            test_df_prophet = test_df.rename(columns={'ë‚ ì§œ': 'ds'})

            model = Prophet()
            model.add_country_holidays(country_name='KOR')
            model.add_regressor('ìš”ì¼')
            model.add_regressor('ê³µíœ´ì¼')

            model.fit(df_prophet)

            # ì„±ëŠ¥ í‰ê°€ìš© ì˜ˆì¸¡ (ë§ˆì§€ë§‰ 30ì¼)
            predict_test = model.predict(test_df_prophet)
            test_predictions = predict_test[['ds', 'yhat']].rename(columns={'ds': 'ë‚ ì§œ', 'yhat': 'ì˜ˆì¸¡ ë§¤ì¶œ'})
            
            # ì„±ëŠ¥ í‰ê°€ (MAPE, RMSE)
            y_true = test_df['ë§¤ì¶œ'].values
            y_pred = test_predictions.loc[test_predictions['ë‚ ì§œ'].isin(test_df['ë‚ ì§œ'])]['ì˜ˆì¸¡ ë§¤ì¶œ'].values
            mape_score = mean_absolute_percentage_error(y_true, y_pred)
            rmse_score = mean_squared_error(y_true, y_pred) ** 0.5 


            ### ëª¨ë“  ë°ì´í„° ì‚¬ìš©í•˜ì—¬ 30ì¼ ì˜ˆì¸¡ ###
            full_train_df = daily_sales_df.copy()
            full_df_prophet = full_train_df.rename(columns={'ë‚ ì§œ': 'ds', 'ë§¤ì¶œ': 'y'})
            
            final_model = Prophet()
            final_model.add_country_holidays(country_name='KOR')
            final_model.add_regressor('ìš”ì¼')
            final_model.add_regressor('ê³µíœ´ì¼')

            final_model.fit(full_df_prophet)

            # í–¥í›„ 30ì¼ ì˜ˆì¸¡
            future = final_model.make_future_dataframe(periods=30)
            future['ìš”ì¼'] = future['ds'].dt.dayofweek
            future['ê³µíœ´ì¼'] = future['ds'].apply(lambda x: 1 if x in kr_holidays else 0)
        
            final_predict = final_model.predict(future)
            predict_df = final_predict[['ds', 'yhat']].rename(columns={'ds': 'ë‚ ì§œ', 'yhat': 'ì˜ˆì¸¡ ë§¤ì¶œ'})

            seasonal_effects = final_predict[['ds', 'trend']] #, 'yearly', 'weekly']]
            seasonal_effects = seasonal_effects.rename(columns={'ds': 'ë‚ ì§œ'})
                
            # ë‚ ì§œ í˜•ì‹ ë³€í™˜
            predict_df['ë‚ ì§œ'] = predict_df['ë‚ ì§œ'].dt.strftime('%Y%m%d')
            predict_df['ì˜ˆì¸¡ ë§¤ì¶œ'] = predict_df['ì˜ˆì¸¡ ë§¤ì¶œ'].round(2)

            # ì˜ˆì¸¡ ì „ 30ì¼ ì‹¤ì œ ë§¤ì¶œ ë°ì´í„°
            recent_30_df = daily_sales_df.tail(30)[['ë‚ ì§œ', 'ë§¤ì¶œ']].copy()
            recent_30_df['ë‚ ì§œ'] = recent_30_df['ë‚ ì§œ'].dt.strftime('%Y%m%d')

            # ì˜ˆì¸¡ ë§ˆì§€ë§‰ 30ì¼ë§Œ ë¶„ë¦¬
            forecast_30 = predict_df.tail(30)

            # ìš”ì•½ ê³„ì‚°
            total_sales = forecast_30["ì˜ˆì¸¡ ë§¤ì¶œ"].sum()
            max_row = forecast_30.loc[forecast_30["ì˜ˆì¸¡ ë§¤ì¶œ"].idxmax()]
            min_row = forecast_30.loc[forecast_30["ì˜ˆì¸¡ ë§¤ì¶œ"].idxmin()]

            # print('================= ì‹¤ì œ ë°ì´í„° ===================')
            # print(recent_30_df)
            # print('================= ì˜ˆì¸¡ ë°ì´í„° ===================')
            # print(forecast_30)

            return {
                "message": "í–¥í›„ 30ì¼ ë§¤ì¶œ ì˜ˆì¸¡ ì™„ë£Œ",

                "previous_30_days": recent_30_df.to_dict(orient='records'), # ì˜ˆì¸¡ ì „ 30ì¼ ì‹¤ì œ ë§¤ì¶œ ë°ì´í„° (ë‚ ì§œ, ë§¤ì¶œ)

                "predictions": forecast_30.to_dict(orient='records'),  

                "summary": {
                    "total_sales": float(total_sales),
                    "average_daily_sales": round(total_sales / 30, 2),
                    "max_sales": {
                        "date": str(max_row["ë‚ ì§œ"]),
                        "value": float(max_row["ì˜ˆì¸¡ ë§¤ì¶œ"])
                    },
                    "min_sales": {
                        "date": str(min_row["ë‚ ì§œ"]),
                        "value": float(min_row["ì˜ˆì¸¡ ë§¤ì¶œ"])
                    }
                },

                "performance": {
                    "mape": round(mape_score, 4),
                    "rmse": round(rmse_score, 4)
                },

                "seasonal_trend": seasonal_effects.to_dict(orient='records') 
            }
        except Exception as e:
            return {"error": str(e)}

    async def cluster_items(self, df: pd.DataFrame):
        """ìƒí’ˆ í´ëŸ¬ìŠ¤í„°ë§"""
        try:
            cluster_df = df[['ìƒí’ˆ ëª…ì¹­', 'ë§¤ì¶œ', 'ë‹¨ê°€', 'ìˆ˜ëŸ‰', 'ì›”', 'ìš”ì¼', 'ì‹œê°„ëŒ€', 'ê³„ì ˆ', 'ê³µíœ´ì¼']] 

            # ë²”ì£¼í˜• ë³€ìˆ˜ë³„ ìˆ˜ëŸ‰ í•©ê³„ í”¼ë²— í…Œì´ë¸” ìƒì„±
            category_vars = ['ì›”', 'ìš”ì¼', 'ì‹œê°„ëŒ€', 'ê³„ì ˆ', 'ê³µíœ´ì¼']
            pivot_tables = [cluster_df.pivot_table(index='ìƒí’ˆ ëª…ì¹­', columns=col, values='ìˆ˜ëŸ‰', aggfunc='sum', fill_value=0) for col in category_vars]
            agg_df = cluster_df.groupby('ìƒí’ˆ ëª…ì¹­').agg({'ë§¤ì¶œ': 'sum', 'ìˆ˜ëŸ‰': 'sum', 'ë‹¨ê°€': 'mean'}) # ë§¤ì¶œê³¼ ìˆ˜ëŸ‰ì€ sum(), ë‹¨ê°€ëŠ” mean()ìœ¼ë¡œ ì§‘ê³„

            # ëª¨ë“  í”¼ë²— í…Œì´ë¸”ì„ ìƒí’ˆëª… ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
            final_df = agg_df.copy()
            for pivot in pivot_tables:
                final_df = final_df.merge(pivot, on='ìƒí’ˆ ëª…ì¹­', how='left')
            final_df.reset_index(inplace=True)

            # ìƒí’ˆëª… ì œê±° í›„ ì •ê·œí™”
            X = final_df.drop(columns=['ìƒí’ˆ ëª…ì¹­'])
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)

            # ìµœì ì˜ k ì°¾ê¸° 
            best_k = self.find_best_k(X_scaled)

            # KMeans í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
            kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
            final_df['Cluster'] = kmeans.fit_predict(X_scaled)

            # í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„ ìš”ì•½ + ëŒ€í‘œ ìƒí’ˆ ì¶”ì¶œ # TODO: ë²”ì£¼í˜• ë³€ìˆ˜ë•Œë¬¸ì— í†µê³„ ìš”ì•½ì„ ë³´ë‚¼ì§€ ê³ ë¯¼í•´ë³´ê¸°
            cluster_summary_df = final_df.groupby("Cluster").agg({
                    "ë§¤ì¶œ": "mean",
                    "ìˆ˜ëŸ‰": "mean",
                    "ë‹¨ê°€": "mean"
                }).reset_index()
            
            representative_items = (
                final_df.groupby("Cluster")
                .apply(lambda x: x.sort_values("ë§¤ì¶œ", ascending=False)["ìƒí’ˆ ëª…ì¹­"].head(3).tolist())
                .reset_index()
                .rename(columns={0: "representative_items"})
            )
            cluster_summary = pd.merge(cluster_summary_df, representative_items, on="Cluster")
            cluster_output = cluster_summary.to_dict(orient="records")

            # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë°ì´í„°
            clusters_dict = (
                final_df.groupby("Cluster")["ìƒí’ˆ ëª…ì¹­"]
                .apply(list)
                .to_dict()
            )
            # keyë¥¼ "cluster0", "cluster1", ... í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            clusters = {f"cluster{cluster_id}": items for cluster_id, items in clusters_dict.items()}

            clusters_full = final_df.to_dict(orient="records")

            return {
                "message": "ìƒí’ˆ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ",
                "optimal_k": best_k,
                "clusters": clusters,
                "cluster_summary": cluster_output,
                "clusters_full": final_df.to_dict(orient="records"),
            }
        
        except Exception as e:
            return {"error": str(e)}

    async def perform_analyze_local(self, local_file_paths: list, store_id: int = 9999, pos_type: str = "í‚¤ì›€"):
        """
        ë¡œì»¬ ì—‘ì…€ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ S3 ì—†ì´ ìë™ ë¶„ì„ ìˆ˜í–‰ (í…ŒìŠ¤íŠ¸ìš©)
        """
        try:
            if pos_type not in self.pos_col:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” POS ìœ í˜•ì…ë‹ˆë‹¤: {pos_type}")
            
            preprocessed_data = []

            for path in local_file_paths:
                script_dir = os.path.dirname(os.path.abspath(__file__))  
                path = os.path.join(script_dir, path)
                df = await self.read_file(path, pos_type)
                df = await self.preprocess_data(df, pos_type)
                preprocessed_data.append(df)

            combined_df = pd.concat(preprocessed_data, ignore_index=True)

            # ì˜ˆì¸¡ & í´ëŸ¬ìŠ¤í„°ë§
            predict_result = await self.predict_next_30_sales(combined_df)
            cluster_result = await self.cluster_items(combined_df)

            # ìš”ì•½
            predict_summary = await autoanalysis_chat_service.generate_sales_predict_summary(predict_result)
            cluster_summary = await autoanalysis_chat_service.generate_cluster_summary(cluster_result)

            return {
                "status": "success",
                "store_id": store_id,
                "results": {
                    "predict": predict_result,
                    "cluster": cluster_result
                },
                "summaries": {
                    "predict_summary": predict_summary,
                    "cluster_summary": cluster_summary
                }
            }
        
        except Exception as e:
            logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise ValueError(f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

    
    async def perform_analyze(self, store_id, source_ids, pos_type):
        """ì—¬ëŸ¬ source_idì˜ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì˜ˆì¸¡ + í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ë°˜í™˜ ë° ì €ì¥"""
        try:
            if pos_type not in self.pos_col:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” POS ìœ í˜•ì…ë‹ˆë‹¤: {pos_type}")
            
            data_sources = mongo_instance.get_collection("DataSources")
            analysis_results = mongo_instance.get_collection("AnalysisResults")

            preprocessed_data = []
            local_files = []
            s3_keys = []

            for source_id in source_ids:
                source = data_sources.find_one({"_id": ObjectId(source_id), "store_id": store_id, "status": "active"})

                if not source:
                    raise ValueError(f"IDê°€ {source_id}ì¸ ìœ íš¨í•œ ë°ì´í„°ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                s3_key = source.get("file_path")
                filename = source.get("original_filename") or s3_key.split("/")[-1]
                s3_keys.append(s3_key)

                temp_path = os.path.join(self.temp_dir, f"{datetime.now().strftime('%Y%m%d%H%M%S')}_{filename}")
                local_path = await download_file_from_s3(s3_key, temp_path)
                local_files.append(local_path)

                df = await self.read_file(local_path, pos_type)
                df = await self.preprocess_data(df, pos_type)
                preprocessed_data.append(df)

            # ëª¨ë“  ì „ì²˜ë¦¬ëœ ë°ì´í„° ë³‘í•©
            combined_df = pd.concat(preprocessed_data, ignore_index=True)

            # ë¶„ì„ ì‹¤í–‰
            predict_result = await self.predict_next_30_sales(combined_df)
            cluster_result = await self.cluster_items(combined_df)

            # ìš”ì•½ ìƒì„± ì¶”ê°€
            predict_summary = await autoanalysis_chat_service.generate_sales_predict_summary(predict_result)
            cluster_summary = await autoanalysis_chat_service.generate_cluster_summary(cluster_result)

            status = "fail" if ("error" in predict_result or "error" in cluster_result) else "completed"

            result_doc = {
                "_id": ObjectId(),
                "store_id": store_id,
                "source_ids": [ObjectId(sid) for sid in source_ids],
                "analysis_type": "autoanalysis",
                "status": status,
                "created_at": datetime.now(),
                "results": {
                    "predict": predict_result,
                    "cluster": cluster_result
                },
                "summaries": {
                    "predict_summary": predict_summary,
                    "cluster_summary": cluster_summary
                }
            }

            result_id = analysis_results.insert_one(result_doc).inserted_id

            data_sources.update_many(
                {"_id": {"$in": [ObjectId(sid) for sid in source_ids]}},
                {"$set": {"last_analyzed": datetime.now()}}
            )

            print()

            return {
                "status": "success",
                "message": "AutoAnalysis ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "result_id": str(result_id),
                "store_id": store_id,
                "source_ids": source_ids,
                "s3_keys": s3_keys,
                "results": {
                    "predict": predict_result,
                    "cluster": cluster_result
                },
                "summaries": {
                    "predict_summary": predict_summary,
                    "cluster_summary": cluster_summary
                }
            }

        except Exception as e:
            logger.error(f"POS ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise ValueError(f"POS ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

        finally:
            for path in local_files:
                if os.path.exists(path):
                    os.remove(path)

autoanalysis_service = AutoAnalysisService()


import asyncio
# from services.automl import AutoAnalysisService

async def test_local_auto_analysis():
    service = AutoAnalysisService()

    # íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° ë°ì´í„° ë¡œë“œ
    # script_dir = os.path.dirname(os.path.abspath(__file__))  
    # temp_file = os.path.join(script_dir, "ì˜ìˆ˜ì¦ ë‚´ì—­(1ì›”~).xlsx")

    # í…ŒìŠ¤íŠ¸í•  ë¡œì»¬ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
    local_file_paths = [
        "í† ìŠ¤í¬ìŠ¤.xlsx"
    ]
    pos_type = "í† ìŠ¤"
    

    result = await service.perform_analyze_local(local_file_paths=local_file_paths, pos_type=pos_type)
    
    print("âœ… ìë™ ë¶„ì„ ìˆ˜í–‰ ì™„ë£Œ")
    print("ğŸ”¸ ì˜ˆì¸¡ ìš”ì•½:", result["summaries"]["predict_summary"])
    print("ğŸ”¸ í´ëŸ¬ìŠ¤í„° ìš”ì•½:", result["summaries"]["cluster_summary"])
    print("ğŸ”¸ ì˜ˆì¸¡ ê²°ê³¼ ì¼ë¶€:", result["results"]["predict"]["predictions"][:3])
    print("ğŸ”¸ í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ì¼ë¶€:", result["results"]["cluster"]["clusters"][:3])

if __name__ == "__main__":
    asyncio.run(test_local_auto_analysis())
